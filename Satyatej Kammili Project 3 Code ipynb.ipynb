{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3: Markov Decision Process Navigation\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Group - 34\n",
        "\n",
        "Name: Satya Tej Kammili\n",
        "\n",
        "Student ID: 11911161"
      ],
      "metadata": {
        "id": "JRoAYwlgoKnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENVIRONMENT SETUP"
      ],
      "metadata": {
        "id": "vfoRslA_Qsa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Grid Size**"
      ],
      "metadata": {
        "id": "fTzIMGNVQvf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROWS = 5\n",
        "COLS = 8\n",
        "\n",
        "print(\"Grid size:\", ROWS, \"rows x\", COLS, \"columns\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiIL7sFCQzNf",
        "outputId": "e7ca4bb4-0651-44b9-f60a-04b372ee23e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid size: 5 rows x 8 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Goal Cell, Hazards and Rewards**\n",
        "\n",
        "* Goal (green cell) is at (row=1, col=3)\n",
        "\n",
        "* Hazards (parked cars) are in column 1, at rows 3 and 4\n",
        "\n",
        "* Hazard reward = -R\n",
        "\n",
        "* Goal reward = +R\n",
        "\n",
        "* R = 1000"
      ],
      "metadata": {
        "id": "SOu2kC4HRAli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt2HKVNQQDe7",
        "outputId": "dbf22a5b-3b87-4f34-f0ed-8b5141ef5f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goal cell: (1, 3)\n",
            "Hazard cells: [(3, 1), (4, 1)]\n",
            "Goal reward: 1000\n",
            "Hazard penalty: -1000\n"
          ]
        }
      ],
      "source": [
        "R = 1000\n",
        "\n",
        "GOAL = (1, 3)\n",
        "\n",
        "HAZARDS = [(3, 1), (4, 1)]\n",
        "\n",
        "GOAL_REWARD = R\n",
        "HAZARD_REWARD = -R\n",
        "\n",
        "print(\"Goal cell:\", GOAL)\n",
        "print(\"Hazard cells:\", HAZARDS)\n",
        "print(\"Goal reward:\", GOAL_REWARD)\n",
        "print(\"Hazard penalty:\", HAZARD_REWARD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Direction Names**\n",
        "\n",
        "* N, NE, E, SE, S, SW, W, NW"
      ],
      "metadata": {
        "id": "fT7rsVHQRgKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIRECTION_NAMES = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n",
        "\n",
        "print(\"Directions:\", DIRECTION_NAMES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxI7Wd7vRkdr",
        "outputId": "522d2cca-5535-456b-d239-297ba8d68489"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directions: ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Direction Movement Offsets (dx, dy)**\n",
        "\n",
        "These come from standard grid movement rules:\n",
        "\n",
        "* N ‚Üí (row +1, col +0)\n",
        "\n",
        "* NE ‚Üí (row +1, col +1)\n",
        "\n",
        "* E ‚Üí (row +0, col +1)\n",
        "\n",
        "* SE ‚Üí (row -1, col +1)\n",
        "\n",
        "* S ‚Üí (row -1, col +0)\n",
        "\n",
        "* SW ‚Üí (row -1, col -1)\n",
        "\n",
        "* W ‚Üí (row +0, col -1)\n",
        "\n",
        "* NW ‚Üí (row +1, col -1)"
      ],
      "metadata": {
        "id": "fAwvpa1bRxFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DIR_OFFSETS = {\n",
        "    \"N\" :  (1, 0),\n",
        "    \"NE\":  (1, 1),\n",
        "    \"E\" :  (0, 1),\n",
        "    \"SE\": (-1, 1),\n",
        "    \"S\" : (-1, 0),\n",
        "    \"SW\": (-1, -1),\n",
        "    \"W\" :  (0, -1),\n",
        "    \"NW\": (1, -1)\n",
        "}\n",
        "\n",
        "print(\"Direction offsets:\")\n",
        "for d, off in DIR_OFFSETS.items():\n",
        "    print(f\"{d}: {off}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_lcFBAAR5RI",
        "outputId": "b36ba5e5-0d20-4126-81b1-eaa2e051d600"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direction offsets:\n",
            "N: (1, 0)\n",
            "NE: (1, 1)\n",
            "E: (0, 1)\n",
            "SE: (-1, 1)\n",
            "S: (-1, 0)\n",
            "SW: (-1, -1)\n",
            "W: (0, -1)\n",
            "NW: (1, -1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define in_bounds()**\n",
        "\n",
        "This function checks whether a cell exists inside the 5√ó8 grid.\n",
        "\n",
        "* Legal row range: 1 ‚Üí 5\n",
        "* Legal col range: 1 ‚Üí 8\n",
        "\n",
        "If a move goes outside, the robot stays in place, as described in edge cases."
      ],
      "metadata": {
        "id": "nl5JL_9WSJoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def in_bounds(cell):\n",
        "    r, c = cell\n",
        "    return 1 <= r <= ROWS and 1 <= c <= COLS\n",
        "\n",
        "print(\"In bounds (3,3):\", in_bounds((3,3)))\n",
        "print(\"In bounds (0,5):\", in_bounds((0,5)))\n",
        "print(\"In bounds (6,1):\", in_bounds((6,1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlj6tiGESSNz",
        "outputId": "0a661b26-014d-4852-a946-e5041b376709"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In bounds (3,3): True\n",
            "In bounds (0,5): False\n",
            "In bounds (6,1): False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the MOVE function**\n",
        "\n",
        "This function:\n",
        "\n",
        "* Takes a state (row, col)\n",
        "\n",
        "* Takes an intended direction\n",
        "\n",
        "* Uses the direction offset from STEP 0.4\n",
        "\n",
        "* Computes the new cell\n",
        "\n",
        "* If the cell is out of bounds ‚Üí the robot stays in place\n",
        "\n",
        "This function will be used inside value iteration, transition probabilities, path checking, and policy generation."
      ],
      "metadata": {
        "id": "-X3GKNn5Slhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Movement Function**"
      ],
      "metadata": {
        "id": "mQOdDV_BSyWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move(state, direction):\n",
        "    \"\"\"Return new state after moving in the given direction.\"\"\"\n",
        "    dr, dc = DIR_OFFSETS[direction]\n",
        "    nr, nc = state[0] + dr, state[1] + dc\n",
        "\n",
        "\n",
        "    if not in_bounds((nr, nc)):\n",
        "        return state\n",
        "\n",
        "    return (nr, nc)\n",
        "\n",
        "\n",
        "print(\"Move (3,3) North:\", move((3,3), \"N\"))\n",
        "print(\"Move (5,3) North:\", move((5,3), \"N\"))\n",
        "print(\"Move (1,1) SW:\", move((1,1), \"SW\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETM4_TElSzZ0",
        "outputId": "438cee1b-ab90-4a27-cc55-93df75370a72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Move (3,3) North: (4, 3)\n",
            "Move (5,3) North: (5, 3)\n",
            "Move (1,1) SW: (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This proves that:\n",
        "\n",
        "\n",
        "* Moving N from the top row ‚Üí stays in place\n",
        "\n",
        "* Moving SW from bottom-left ‚Üí stays in place\n",
        "\n",
        "* Movement inside bounds works correctly"
      ],
      "metadata": {
        "id": "bUAF-t9dTGQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Permitted Movement Rule\n",
        "\n",
        "This rule determines:\n",
        "\n",
        "* Which directions the robot is allowed to move\n",
        "\n",
        "* Which directions get removed because they are 90¬∞ or 180¬∞ away\n",
        "\n",
        "* How to handle edge cases"
      ],
      "metadata": {
        "id": "lk49nF9zTVqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLOCKWISE direction order**"
      ],
      "metadata": {
        "id": "y5OY0e2ITl9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLOCKWISE = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n",
        "\n",
        "print(\"Clockwise direction order:\", CLOCKWISE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxJYCbIDTIy4",
        "outputId": "ae04b64d-b8e7-4d1f-99a2-a3b2244bea15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clockwise direction order: ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identify which directions are 90¬∞ or 180¬∞ away**\n",
        "\n",
        "* Method is page with movement rules\n",
        "\n",
        "\n",
        "- Start scanning the clockwise list at the intended direction.\n",
        "* Drop every other direction to eliminate 90¬∞ and 180¬∞ directions.\n",
        "\n",
        "\n",
        "If intended direction = NE\n",
        "Scanning clockwise list starting at NE:"
      ],
      "metadata": {
        "id": "bgX-dpCnUALc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to get (90¬∞ + 180¬∞) blocked directions**"
      ],
      "metadata": {
        "id": "Sak5kWbKUatn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blocked_directions(intended):\n",
        "    \"\"\"\n",
        "    Return the correct set of 90¬∞ and 180¬∞ blocked directions\n",
        "    using the Tutorial 5 permitted-move table.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    permitted = {\n",
        "        \"N\" :  {\"N\", \"NE\", \"NW\", \"SE\", \"SW\"},\n",
        "        \"S\" :  {\"S\", \"SE\", \"SW\", \"NE\", \"NW\"},\n",
        "        \"E\" :  {\"E\", \"NE\", \"SE\", \"N\", \"S\"},\n",
        "        \"W\" :  {\"W\", \"NW\", \"SW\", \"N\", \"S\"},\n",
        "        \"NE\":  {\"NE\", \"N\", \"E\", \"S\", \"W\"},\n",
        "        \"NW\":  {\"NW\", \"N\", \"W\", \"S\", \"E\"},\n",
        "        \"SE\":  {\"SE\", \"S\", \"E\", \"N\", \"W\"},\n",
        "        \"SW\":  {\"SW\", \"S\", \"W\", \"N\", \"E\"}\n",
        "    }\n",
        "\n",
        "    allowed = permitted[intended]\n",
        "    blocked = set(DIRECTION_NAMES) - allowed\n",
        "\n",
        "    return blocked\n",
        "\n",
        "\n",
        "print(\"Blocked for N:\", get_blocked_directions(\"N\"))\n",
        "print(\"Blocked for NE:\", get_blocked_directions(\"NE\"))\n",
        "print(\"Blocked for E:\", get_blocked_directions(\"E\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf7Au8X_VsEr",
        "outputId": "f44eddd8-3674-4a56-aebe-a5819fae6843"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blocked for N: {'E', 'S', 'W'}\n",
            "Blocked for NE: {'NW', 'SW', 'SE'}\n",
            "Blocked for E: {'NW', 'SW', 'W'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification**\n",
        "\n",
        "\n",
        "Blocked for N should be: {E, W, S}\n",
        "* My result: {'E', 'S', 'W'}\n",
        "\n",
        "Blocked for NE should be: {NW, SE, SW}\n",
        "* My result: {'NW', 'SW', 'SE'}\n",
        "\n",
        "Blocked for E should be: {W, NW, SW}\n",
        "* My result: {'NW', 'SW', 'W'}\n",
        "\n",
        "This means the permitted movement table is correct, and I can now safely build the transition system"
      ],
      "metadata": {
        "id": "Gnl6yQ3XV9Ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle Blocked Moves (s' = s)**\n",
        "\n",
        "\n",
        "* If the intended direction results in a blocked position,\n",
        "replace the intended move with staying in place (s‚Äô = s).\n",
        "\n",
        "So for example:\n",
        "\n",
        "* If you're in the top row (row 5), and you try to move ‚ÄúN‚Äù, it becomes a stay-in-place move.\n",
        "\n",
        "* If NE is blocked because of boundary or obstacle, NE becomes ‚Äústay at same cell‚Äù."
      ],
      "metadata": {
        "id": "ea_0uiA3WcfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply Blocked Move Rule**"
      ],
      "metadata": {
        "id": "vqvtgG3LWpPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_block_rule(state, intended_direction):\n",
        "\n",
        "    blocked_dirs = get_blocked_directions(intended_direction)\n",
        "\n",
        "    if intended_direction in blocked_dirs:\n",
        "        return state\n",
        "\n",
        "\n",
        "    new_state = move(state, intended_direction)\n",
        "\n",
        "\n",
        "    if new_state == state:\n",
        "        return state\n",
        "\n",
        "    return new_state\n",
        "\n",
        "print(\"Try moving N from top row (5,3):\", apply_block_rule((5,3), \"N\"))\n",
        "print(\"Try moving W from leftmost col (3,1):\", apply_block_rule((3,1), \"W\"))\n",
        "print(\"Try NE from regular cell (3,3):\", apply_block_rule((3,3), \"NE\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4iFR6ZxV_D_",
        "outputId": "0a157577-37c2-486e-e636-126b5a03df76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Try moving N from top row (5,3): (5, 3)\n",
            "Try moving W from leftmost col (3,1): (3, 1)\n",
            "Try NE from regular cell (3,3): (4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output**\n",
        "\n",
        "* N from (5,3) wil stays (top row)\n",
        "\n",
        "* W from (3,1) wil stays (left col)\n",
        "\n",
        "* NE from (3,3) will moves to (4,4)"
      ],
      "metadata": {
        "id": "k2a9I8sUXanO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permitted Directions\n",
        "\n",
        "**Define PERMITTED_DIRS + helper**"
      ],
      "metadata": {
        "id": "bBftFiclXkAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PERMITTED_DIRS = {\n",
        "    \"N\" :  {\"N\", \"NE\", \"NW\", \"SE\", \"SW\"},\n",
        "    \"S\" :  {\"S\", \"SE\", \"SW\", \"NE\", \"NW\"},\n",
        "    \"E\" :  {\"E\", \"NE\", \"SE\", \"N\", \"S\"},\n",
        "    \"W\" :  {\"W\", \"NW\", \"SW\", \"N\", \"S\"},\n",
        "    \"NE\":  {\"NE\", \"N\", \"E\", \"S\", \"W\"},\n",
        "    \"NW\":  {\"NW\", \"N\", \"W\", \"S\", \"E\"},\n",
        "    \"SE\":  {\"SE\", \"S\", \"E\", \"N\", \"W\"},\n",
        "    \"SW\":  {\"SW\", \"S\", \"W\", \"N\", \"E\"}\n",
        "}\n",
        "\n",
        "def get_permitted_directions(intended):\n",
        "    return PERMITTED_DIRS[intended]\n",
        "\n",
        "def get_blocked_directions(intended):\n",
        "    allowed = PERMITTED_DIRS[intended]\n",
        "    return set(DIRECTION_NAMES) - allowed\n",
        "\n",
        "print(\"Permitted for N:\", get_permitted_directions(\"N\"))\n",
        "print(\"Blocked for N:\", get_blocked_directions(\"N\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaPIsHZYXssz",
        "outputId": "0539e430-3893-4408-bec7-0d96416b4b72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Permitted for N: {'SE', 'NW', 'SW', 'NE', 'N'}\n",
            "Blocked for N: {'E', 'S', 'W'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transition Probabilities - MDP core**\n",
        "\n",
        "Now we build:\n",
        "\n",
        "ùëá\n",
        "(\n",
        "ùë†\n",
        ",\n",
        "ùëé\n",
        "‚Üí\n",
        "ùë†\n",
        "‚Ä≤\n",
        ") =\n",
        "ùëÉ\n",
        "(\n",
        "ùë†\n",
        "‚Ä≤\n",
        "‚à£\n",
        "ùë†\n",
        ",\n",
        "ùëé\n",
        ")\n",
        "\n",
        "\n",
        "Using project settings:\n",
        "\n",
        "* Noise = 0.1\n",
        "\n",
        "* Intended direction probability = 0.9\n",
        "\n",
        "* Remaining 0.1 is shared between other permitted directions"
      ],
      "metadata": {
        "id": "IrTlVojCYJKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define transition probability function**"
      ],
      "metadata": {
        "id": "D4YrxKH8Ypxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE = 0.1\n",
        "INTENDED_PROB = 1.0 - NOISE\n",
        "\n",
        "def get_transitions(state, intended_dir, obstacles=None):\n",
        "    \"\"\"\n",
        "    Return list of (next_state, probability) given current state and intended direction.\n",
        "    obstacles: optional set of blocked cells (e.g., {(4,3)} in R2).\n",
        "    \"\"\"\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "\n",
        "    allowed_dirs = list(get_permitted_directions(intended_dir))\n",
        "\n",
        "\n",
        "    unintended_dirs = [d for d in allowed_dirs if d != intended_dir]\n",
        "\n",
        "    transitions = {}\n",
        "\n",
        "\n",
        "    intended_next = move(state, intended_dir)\n",
        "\n",
        "\n",
        "    if intended_next in obstacles or intended_next == state:\n",
        "        intended_next = state\n",
        "\n",
        "    transitions[intended_next] = transitions.get(intended_next, 0) + INTENDED_PROB\n",
        "\n",
        "\n",
        "    if len(unintended_dirs) > 0:\n",
        "        p_unintended = NOISE / len(unintended_dirs)\n",
        "\n",
        "        for d in unintended_dirs:\n",
        "            ns = move(state, d)\n",
        "            if ns in obstacles:\n",
        "                ns = state\n",
        "            transitions[ns] = transitions.get(ns, 0) + p_unintended\n",
        "\n",
        "\n",
        "    result = [(s, p) for s, p in transitions.items()]\n",
        "    return result\n",
        "\n",
        "\n",
        "test_state = (3, 3)\n",
        "print(\"Transitions from\", test_state, \"with intended N:\")\n",
        "for ns, pr in get_transitions(test_state, \"N\"):\n",
        "    print(\"  ->\", ns, \"with p =\", pr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCkGBGsyYrNd",
        "outputId": "3c015e65-1346-4e78-922c-20773bef0d4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transitions from (3, 3) with intended N:\n",
            "  -> (4, 3) with p = 0.9\n",
            "  -> (2, 4) with p = 0.025\n",
            "  -> (4, 2) with p = 0.025\n",
            "  -> (2, 2) with p = 0.025\n",
            "  -> (4, 4) with p = 0.025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We now have:**\n",
        "\n",
        "* Correct permitted directions\n",
        "* Correct blocked directions\n",
        "* Correct movement\n",
        "* Correct handling of obstacles\n",
        "* Correct transition probabilities\n",
        "* Correct noise split 0.1 / 0.025\n",
        "* Correct staying-in-place logic"
      ],
      "metadata": {
        "id": "j4QDstXDZCFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Iteration (MDP)\n",
        "\n",
        "\n",
        "This is the algorithm that computes the utilities of each cell and extracts the optimal policy."
      ],
      "metadata": {
        "id": "Cpng3BNPZJE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize utility grid U(s)**\n",
        "\n",
        "Utility starts as 0 for all cells, except:\n",
        "\n",
        "* hazards is -1000\n",
        "\n",
        "* goal is +1000\n",
        "\n",
        "\n",
        "If obstacles exist (in R2), those cells will simply be unreachable during transitions."
      ],
      "metadata": {
        "id": "O1XWEF1hZUJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create U(s) initial matrix**"
      ],
      "metadata": {
        "id": "aJZk1V6LZg-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_utilities(obstacles=None):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "    U = {}\n",
        "    for r in range(1, ROWS+1):\n",
        "        for c in range(1, COLS+1):\n",
        "            cell = (r,c)\n",
        "            if cell == GOAL:\n",
        "                U[cell] = GOAL_REWARD\n",
        "            elif cell in HAZARDS:\n",
        "                U[cell] = HAZARD_REWARD\n",
        "            else:\n",
        "                U[cell] = 0\n",
        "    return U\n",
        "\n",
        "\n",
        "U0 = initialize_utilities()\n",
        "print(\"Initial utilities for some key cells:\")\n",
        "print(\"Goal:\", U0[GOAL])\n",
        "print(\"Hazard 1:\", U0[HAZARDS[0]])\n",
        "print(\"Hazard 2:\", U0[HAZARDS[1]])\n",
        "print(\"Random cell (2,5):\", U0[(2,5)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAZ6B7_UZh80",
        "outputId": "36445418-4fe6-4167-bea7-bda7712fa401"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial utilities for some key cells:\n",
            "Goal: 1000\n",
            "Hazard 1: -1000\n",
            "Hazard 2: -1000\n",
            "Random cell (2,5): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bellman Update (Core of Value Iteration)**\n",
        "\n",
        "Bellman equation\n",
        "\n",
        "\n",
        "\n",
        "ùëà\n",
        "ùëõ\n",
        "ùëí\n",
        "ùë§\n",
        "(\n",
        "ùë†\n",
        ") = ùëü\n",
        "(\n",
        "ùë†\n",
        ")\n",
        "+\n",
        "ùõæ\n",
        "max\n",
        "‚Å°\n",
        "ùëé\n",
        "‚àë\n",
        "ùë†\n",
        "‚Ä≤\n",
        "ùëá\n",
        "(\n",
        "ùë†\n",
        ",\n",
        "ùëé\n",
        ",\n",
        "ùë†\n",
        "‚Ä≤\n",
        ")\n",
        "‚ãÖ\n",
        "ùëà\n",
        "(\n",
        "ùë†\n",
        "‚Ä≤\n",
        ")\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "* r(s) = reward for state\n",
        "\n",
        "* Œ≥ = discount factor (we use 0.99 unless changed by professor)\n",
        "\n",
        "* T(s,a,s‚Ä≤) = transition probability\n",
        "\n",
        "* max over actions = best action at state\n",
        "\n",
        "Value iteration uses this update repeatedly until convergence.\n",
        "\n",
        "I implement a single Bellman update in a helper function:"
      ],
      "metadata": {
        "id": "yqq5rabcZyMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bellman Update Function**"
      ],
      "metadata": {
        "id": "5C-FdK-5aPv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "GAMMA = 0.99\n",
        "\n",
        "def bellman_update(state, U, living_reward, obstacles=None):\n",
        "\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "\n",
        "    if state == GOAL:\n",
        "        return GOAL_REWARD\n",
        "\n",
        "    if state in HAZARDS:\n",
        "        return HAZARD_REWARD\n",
        "\n",
        "\n",
        "    best_action_value = -999999999\n",
        "\n",
        "    for action in DIRECTION_NAMES:\n",
        "\n",
        "        transitions = get_transitions(state, action, obstacles=obstacles)\n",
        "\n",
        "\n",
        "        exp_value = 0\n",
        "        for (s2, p) in transitions:\n",
        "            exp_value += p * U[s2]\n",
        "\n",
        "\n",
        "        if exp_value > best_action_value:\n",
        "            best_action_value = exp_value\n",
        "\n",
        "\n",
        "    return living_reward + GAMMA * best_action_value\n"
      ],
      "metadata": {
        "id": "yO21CNMtaRwl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this function does**\n",
        "\n",
        "* If the state is goal utility = +1000\n",
        "\n",
        "* If the state is hazard utility = ‚Äì1000\n",
        "\n",
        "* Otherwise:\n",
        "\n",
        "1. Try each action (N, NE, E, ‚Ä¶)\n",
        "\n",
        "2. Compute expected utility using transitions\n",
        "\n",
        "3. Select best action‚Äôs value\n",
        "\n",
        "4. Add living reward r\n",
        "\n",
        "5. Return Bellman-updated utility"
      ],
      "metadata": {
        "id": "tpO5yD7UaiUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value Iteration (full loop)**\n",
        "\n",
        "This step will:\n",
        "\n",
        "* Repeatedly apply Bellman updates\n",
        "\n",
        "* Update the utility values U(s) until convergence\n",
        "\n",
        "* Use the live-in reward r (the value we must test from ‚Äì20 ‚Üí 0)\n",
        "\n",
        "* Produce the final utility grid U*\n",
        "\n",
        "Once value iteration works, extracting the policy P1 is VERY easy."
      ],
      "metadata": {
        "id": "4JjNLjdTaxLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value Iteration Loop**\n",
        "\n",
        "I will create a function:  value_iteration(living_reward, obstacles=None)\n",
        "\n",
        "\n",
        "It will:\n",
        "\n",
        "* Initialize utilities\n",
        "\n",
        "* Loop up to max iterations\n",
        "\n",
        "* For each state, apply Bellman update\n",
        "\n",
        "* Stop when the update is small (delta < 0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "zN99G08Ia_KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value Iteration Function**"
      ],
      "metadata": {
        "id": "pQwUyXQ9bSbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(living_reward, max_iterations=200, tolerance=0.001, obstacles=None):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "\n",
        "    U = initialize_utilities(obstacles=obstacles)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        delta = 0\n",
        "        new_U = U.copy()\n",
        "\n",
        "        for r in range(1, ROWS+1):\n",
        "            for c in range(1, COLS+1):\n",
        "                state = (r,c)\n",
        "                updated_value = bellman_update(state, U, living_reward, obstacles)\n",
        "\n",
        "                delta = max(delta, abs(updated_value - U[state]))\n",
        "                new_U[state] = updated_value\n",
        "\n",
        "        U = new_U\n",
        "\n",
        "\n",
        "        if delta < tolerance:\n",
        "            print(f\"Converged at iteration {iteration}\")\n",
        "            break\n",
        "\n",
        "    return U\n"
      ],
      "metadata": {
        "id": "p-CzrTEybTVp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Optimal Policy P1**\n",
        "\n",
        "This delivers EXACTLY what R1 requires:\n",
        "\n",
        "*  Policy P1 for the environment\n",
        "* Best action for each cell\n",
        "* Works with any living reward r\n",
        "* This will also let us find the FIRST r between ‚Äì20 and 0 that produces a valid policy P1"
      ],
      "metadata": {
        "id": "fKfXPsHzbnx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Best Action for ONE state**\n",
        "\n",
        "\n",
        "For a given state s:\n",
        "\n",
        "* Evaluates every action a\n",
        "\n",
        "* Computes expected utility\n",
        "\n",
        "* Chooses the best action\n",
        "\n",
        "This is basically part of the Bellman equation, but without updating utilities"
      ],
      "metadata": {
        "id": "oeS70iqObzLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best Action for a Single State**"
      ],
      "metadata": {
        "id": "J781f2B5b6Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def best_action_for_state(state, U, obstacles=None):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "\n",
        "    if state == GOAL:\n",
        "        return \"GOAL\"\n",
        "    if state in HAZARDS:\n",
        "        return \"X\"\n",
        "\n",
        "    best_action = None\n",
        "    best_value = -99999999\n",
        "\n",
        "    for action in DIRECTION_NAMES:\n",
        "        transitions = get_transitions(state, action, obstacles)\n",
        "\n",
        "        exp_value = 0\n",
        "        for (s2, p) in transitions:\n",
        "            exp_value += p * U[s2]\n",
        "\n",
        "        if exp_value > best_value:\n",
        "            best_value = exp_value\n",
        "            best_action = action\n",
        "\n",
        "    return best_action\n"
      ],
      "metadata": {
        "id": "NQMzImOXb61X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the FULL POLICY GRID P1**\n",
        "\n",
        "This will produce a grid like:\n",
        "\n",
        "‚Üí  ‚Üí  ‚Üë  ‚Üë\n",
        "\n",
        "‚Üó  ‚Üí  ‚Üë  ‚úì"
      ],
      "metadata": {
        "id": "N6K0gSPlcjvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Value Iteration Once (with a sample r)\n",
        "\n",
        "**Run VI with r = -10**"
      ],
      "metadata": {
        "id": "lCz4iEImcvfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_r = -10\n",
        "\n",
        "U_test = value_iteration(living_reward=test_r)\n",
        "\n",
        "print(\"Sample utilities for a few cells with r =\", test_r)\n",
        "print(\"Goal (1,3):\", U_test[(1,3)])\n",
        "print(\"Hazard (3,1):\", U_test[(3,1)])\n",
        "print(\"Cell (2,2):\", U_test[(2,2)])\n",
        "print(\"Cell (3,3):\", U_test[(3,3)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M34DKW_KcypH",
        "outputId": "6faffa97-a6b4-4441-8de7-0bcdb9fe2b80"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at iteration 18\n",
            "Sample utilities for a few cells with r = -10\n",
            "Goal (1,3): 1000\n",
            "Hazard (3,1): -1000\n",
            "Cell (2,2): 975.6214631339985\n",
            "Cell (3,3): 954.1389102445738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the FULL Policy Grid (P1 for that r)**\n",
        "\n",
        "Once I know U(s), I can compute a policy:\n",
        "\n",
        "* For each cell (r,c), we call best_action_for_state((r,c), U_test)\n",
        "\n",
        "* I can store the action symbol there\n",
        "\n",
        "* For goal: mark as G\n",
        "\n",
        "* For hazards: mark as H\n",
        "\n",
        "Later, I reuse the same code for the ‚Äúreal‚Äù P1 once I find the correct r"
      ],
      "metadata": {
        "id": "zZTK-lccdcg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create & Print Policy Grid**"
      ],
      "metadata": {
        "id": "kdoTTMG3dq7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_policy_grid(U, obstacles=None):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "    policy_grid = []\n",
        "\n",
        "    for r in range(ROWS, 1-1, -1):\n",
        "        row_actions = []\n",
        "        for c in range(1, COLS+1):\n",
        "            state = (r,c)\n",
        "            if state == GOAL:\n",
        "                row_actions.append(\" G \")\n",
        "            elif state in HAZARDS:\n",
        "                row_actions.append(\" H \")\n",
        "            else:\n",
        "                a = best_action_for_state(state, U, obstacles)\n",
        "                if a is None:\n",
        "                    row_actions.append(\" . \")\n",
        "                else:\n",
        "                    row_actions.append(f\"{a:>2}\")\n",
        "        policy_grid.append(row_actions)\n",
        "\n",
        "    return policy_grid\n",
        "\n",
        "def print_policy_grid(policy_grid):\n",
        "    for row in policy_grid:\n",
        "        print(\" | \".join(row))\n",
        "\n",
        "policy_test = build_policy_grid(U_test)\n",
        "print(\"Policy grid for r = -10:\\n\")\n",
        "print_policy_grid(policy_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPfjOtU8dpZf",
        "outputId": "13118338-6a21-4334-ebf3-e3d854b4aa58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy grid for r = -10:\n",
            "\n",
            "SE | SE | SE |  S |  S | SW | SW | SW\n",
            " H  |  E | SE |  S | SW | SW | SW | SW\n",
            " H  |  E | SE | SW | SW | SW | SW | SW\n",
            " S | SE |  S | SW | SW |  W |  W |  W\n",
            " E |  E |  G  |  W |  W |  W |  W | NW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Direction Codes to Arrow Symbols**\n",
        "\n",
        "Supported arrows for 8 directions:\n",
        "\n",
        "* N  ‚Üí ‚Üë\n",
        "\n",
        "* S  ‚Üí ‚Üì\n",
        "\n",
        "* E  ‚Üí ‚Üí\n",
        "\n",
        "* W  ‚Üí ‚Üê\n",
        "\n",
        "* NE ‚Üí ‚Üó\n",
        "\n",
        "* NW ‚Üí ‚Üñ\n",
        "\n",
        "* SE ‚Üí ‚Üò\n",
        "\n",
        "* SW ‚Üí ‚Üô"
      ],
      "metadata": {
        "id": "zDI3oU1geE81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arrow Dictionary + Pretty Printing**"
      ],
      "metadata": {
        "id": "A8wFX1QJeQE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ARROWS = {\n",
        "    \"N\":  \"‚Üë\",\n",
        "    \"S\":  \"‚Üì\",\n",
        "    \"E\":  \"‚Üí\",\n",
        "    \"W\":  \"‚Üê\",\n",
        "    \"NE\": \"‚Üó\",\n",
        "    \"NW\": \"‚Üñ\",\n",
        "    \"SE\": \"‚Üò\",\n",
        "    \"SW\": \"‚Üô\",\n",
        "    \"GOAL\": \" G \",\n",
        "    \"X\": \" H \"\n",
        "}\n",
        "\n",
        "def build_policy_grid_arrows(U, obstacles=None):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "    grid = []\n",
        "\n",
        "\n",
        "    for r in range(ROWS, 0, -1):\n",
        "        row = []\n",
        "        for c in range(1, COLS+1):\n",
        "            state = (r,c)\n",
        "\n",
        "            if state == GOAL:\n",
        "                row.append(\" G \")\n",
        "            elif state in HAZARDS:\n",
        "                row.append(\" H \")\n",
        "            else:\n",
        "                a = best_action_for_state(state, U, obstacles)\n",
        "                if a in ARROWS:\n",
        "                    row.append(f\" {ARROWS[a]} \")\n",
        "                else:\n",
        "                    row.append(\" . \")\n",
        "        grid.append(row)\n",
        "    return grid\n",
        "\n",
        "def print_policy_grid_arrows(grid):\n",
        "    for row in grid:\n",
        "        print(\" \".join(row))\n"
      ],
      "metadata": {
        "id": "cAk3OdTuebJk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now build your arrow-based policy:**"
      ],
      "metadata": {
        "id": "aUGm7O9Sejj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_arrows = build_policy_grid_arrows(U_test)\n",
        "print(\"Policy grid with arrows (r = -10):\\n\")\n",
        "print_policy_grid_arrows(policy_arrows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOt_WAmQekR9",
        "outputId": "928fec8e-27f9-4823-be79-9e5d4a56c3fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy grid with arrows (r = -10):\n",
            "\n",
            " ‚Üò   ‚Üò   ‚Üò   ‚Üì   ‚Üì   ‚Üô   ‚Üô   ‚Üô \n",
            " H   ‚Üí   ‚Üò   ‚Üì   ‚Üô   ‚Üô   ‚Üô   ‚Üô \n",
            " H   ‚Üí   ‚Üò   ‚Üô   ‚Üô   ‚Üô   ‚Üô   ‚Üô \n",
            " ‚Üì   ‚Üò   ‚Üì   ‚Üô   ‚Üô   ‚Üê   ‚Üê   ‚Üê \n",
            " ‚Üí   ‚Üí   G   ‚Üê   ‚Üê   ‚Üê   ‚Üê   ‚Üñ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oberservation:\n",
        "\n",
        "\n",
        "* Goal is correctly attracting everything\n",
        "* Hazards correctly repel movement\n",
        "* Arrows smoothly funnel toward the goal at (1,3)\n",
        "* This matches EXACT behavior expected from VI"
      ],
      "metadata": {
        "id": "ay_PvfA6ew13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R1 REQUIREMENT**\n",
        "\n",
        "\n",
        "* Find the FIRST living reward r in [-20, 0] that produces a successful policy P1\n",
        "A successful policy is one where the agent reaches the goal from every start cell S1..S7 AND the path uses permitted movements"
      ],
      "metadata": {
        "id": "tC_7b75-fGU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loop r = ‚Äì20 to 0 and Find First Valid P1**"
      ],
      "metadata": {
        "id": "AsLnEOxmfT13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Start Cells S1..S7**\n",
        "\n",
        "These are from Figure 1:\n",
        "\n",
        "* S1 = (1,1)\n",
        "* S2 = (2,1)\n",
        "* S3 = (2,2)\n",
        "* S4 = (3,2)\n",
        "* S5 = (4,2)\n",
        "* S6 = (5,2)\n",
        "* S7 = (5,3)"
      ],
      "metadata": {
        "id": "1MJWgXFsffLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Start States**"
      ],
      "metadata": {
        "id": "yceJq6nBfro5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_STATES = [\n",
        "    (1,1),  # S1\n",
        "    (2,1),  # S2\n",
        "    (2,2),  # S3\n",
        "    (3,2),  # S4\n",
        "    (4,2),  # S5\n",
        "    (5,2),  # S6\n",
        "    (5,3)   # S7\n",
        "]\n",
        "\n",
        "print(\"Start states (S1..S7):\", START_STATES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpJMDCB-eypY",
        "outputId": "c75e5a2f-f7ce-44d4-92c9-0175a6ca73bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start states (S1..S7): [(1, 1), (2, 1), (2, 2), (3, 2), (4, 2), (5, 2), (5, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Now I can begin testing whether a policy actually reaches the goal from all S1‚ÄìS7.\n",
        "\n",
        "This is essential for R1, I want the first r value in the range\n",
        "‚àí\n",
        "20\n",
        ",\n",
        "0\n",
        " that produces a working policy.\n",
        "\n",
        "To do that, I need to simulate following the policy until:\n",
        "\n",
        "* The agent reaches the GOAL  success\n",
        "\n",
        "* The agent gets stuck  failure\n",
        "\n",
        "* The agent enters an infinite loop failure\n",
        "\n",
        "2. Simulate Following a Policy : follow_policy(state, policy, max_steps=200)\n",
        "\n",
        "It will:\n",
        "\n",
        "* Start at S1..S7\n",
        "\n",
        "* Follow arrows\n",
        "\n",
        "* Stop if:\n",
        "\n",
        "1. Goal reached\n",
        "\n",
        "2. Stuck (stays in place)\n",
        "\n",
        "3. Exceeds max steps (loop ‚Üí fail)\n",
        "\n",
        "This determines whether a policy is ‚Äúvalid‚Äù."
      ],
      "metadata": {
        "id": "WeO1VL6hgFBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Follow Policy Simulation**"
      ],
      "metadata": {
        "id": "96EABUyTg3KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def follow_policy(start_state, policy, obstacles=None, max_steps=200):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "    state = start_state\n",
        "\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        if state == GOAL:\n",
        "            return True, step\n",
        "\n",
        "\n",
        "        if state in HAZARDS:\n",
        "            return False, step\n",
        "\n",
        "\n",
        "        action = policy[state]\n",
        "\n",
        "\n",
        "        if action not in DIR_OFFSETS:\n",
        "            return False, step\n",
        "\n",
        "\n",
        "        next_state = move(state, action)\n",
        "\n",
        "\n",
        "        if next_state == state:\n",
        "            return False, step\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "    return False, max_steps\n"
      ],
      "metadata": {
        "id": "OonlRDpKg4BC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this does**\n",
        "\n",
        "It will allow me to test:\n",
        "\n",
        "* For each r in range ‚Äì20 to 0\n",
        "\n",
        "* Whether P1 allows reaching the goal from all start states"
      ],
      "metadata": {
        "id": "hJn8R5iOhFDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loop r = ‚Äì20 to 0**\n",
        "\n",
        "For each r:\n",
        "\n",
        "1. Run value iteration\n",
        "\n",
        "2. Build policy\n",
        "\n",
        "3. Test policy from all 7 start states\n",
        "\n",
        "4. Stop when the FIRST successful r is found\n",
        "\n",
        "5. Print:\n",
        "\n",
        "* The correct r\n",
        "\n",
        "* The policy grid (arrows)\n",
        "\n",
        "* Utilities (optional)"
      ],
      "metadata": {
        "id": "xGCyVzpehSCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full R1 Search**"
      ],
      "metadata": {
        "id": "GBi9q_XKhhOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_first_valid_r(obstacles=None):\n",
        "    if obstacles is None:\n",
        "        obstacles = set()\n",
        "\n",
        "    for r in range(-20, 1):\n",
        "        print(f\"\\nTesting r = {r}...\")\n",
        "\n",
        "\n",
        "        U = value_iteration(living_reward=r, obstacles=obstacles)\n",
        "\n",
        "\n",
        "        policy = {}\n",
        "        for rr in range(1, ROWS+1):\n",
        "            for cc in range(1, COLS+1):\n",
        "                s = (rr, cc)\n",
        "                policy[s] = best_action_for_state(s, U, obstacles)\n",
        "\n",
        "\n",
        "        all_success = True\n",
        "        for s in START_STATES:\n",
        "            ok, steps = follow_policy(s, policy, obstacles)\n",
        "            print(f\"  From {s}: {'Reached goal' if ok else 'FAILED'} in {steps} steps\")\n",
        "            if not ok:\n",
        "                all_success = False\n",
        "\n",
        "\n",
        "        if all_success:\n",
        "            print(\"\\nFOUND FIRST VALID r =\", r)\n",
        "\n",
        "\n",
        "            print(\"\\nPolicy grid for r =\", r)\n",
        "            policy_grid = build_policy_grid_arrows(U, obstacles)\n",
        "            print_policy_grid_arrows(policy_grid)\n",
        "\n",
        "            return r, U, policy\n",
        "\n",
        "    print(\"\\nNo valid r found in [-20, 0].\")\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "final_r, final_U, final_policy = find_first_valid_r()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUqEjmBwhiEX",
        "outputId": "3bdc7d57-bc87-4867-80fb-f9a3679eabb3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing r = -20...\n",
            "Converged at iteration 18\n",
            "  From (1, 1): Reached goal in 2 steps\n",
            "  From (2, 1): Reached goal in 3 steps\n",
            "  From (2, 2): Reached goal in 1 steps\n",
            "  From (3, 2): Reached goal in 3 steps\n",
            "  From (4, 2): Reached goal in 4 steps\n",
            "  From (5, 2): Reached goal in 4 steps\n",
            "  From (5, 3): Reached goal in 4 steps\n",
            "\n",
            "FOUND FIRST VALID r = -20\n",
            "\n",
            "Policy grid for r = -20\n",
            " ‚Üò   ‚Üò   ‚Üò   ‚Üì   ‚Üì   ‚Üô   ‚Üô   ‚Üô \n",
            " H   ‚Üí   ‚Üò   ‚Üì   ‚Üô   ‚Üô   ‚Üô   ‚Üô \n",
            " H   ‚Üí   ‚Üò   ‚Üô   ‚Üô   ‚Üô   ‚Üô   ‚Üô \n",
            " ‚Üì   ‚Üò   ‚Üì   ‚Üô   ‚Üô   ‚Üê   ‚Üê   ‚Üê \n",
            " ‚Üí   ‚Üí   G   ‚Üê   ‚Üê   ‚Üê   ‚Üê   ‚Üñ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R2 ‚Äî New Environment with Obstacle at (3,4)**\n",
        "\n",
        "From the spec:\n",
        "\n",
        "‚Äúan obstacle has been introduced at position (3,4)‚Äù\n",
        "\n",
        "\n",
        "* Add that obstacle\n",
        "\n",
        "* Re-run value iteration using the same r = -20\n",
        "\n",
        "* Build P2 (arrow policy)\n",
        "\n",
        "* Check if P2 is same as P1"
      ],
      "metadata": {
        "id": "OoKvT21MiwFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Obstacle for R2 and Run Value Iteration**"
      ],
      "metadata": {
        "id": "nyqnYubni-8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OBSTACLES_R2 = {(3, 4)}\n",
        "\n",
        "print(\"Obstacles in R2:\", OBSTACLES_R2)\n",
        "\n",
        "U_R2 = value_iteration(living_reward=final_r, obstacles=OBSTACLES_R2)\n",
        "\n",
        "print(\"Done computing utilities for R2 environment.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAphZG2ai_sK",
        "outputId": "d8d8385d-79d7-4171-adb6-a8426d344e0f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obstacles in R2: {(3, 4)}\n",
            "Converged at iteration 17\n",
            "Done computing utilities for R2 environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build & Print P2 Policy Grid (Arrows)**\n",
        "\n",
        "* Use the same policy builder I used earlier and giving it U_R2 and OBSTACLES_R2."
      ],
      "metadata": {
        "id": "aHvrUKgpjbr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_R2_grid = build_policy_grid_arrows(U_R2, obstacles=OBSTACLES_R2)\n",
        "\n",
        "print(\"Policy Grid (P2) with obstacle at (3,4):\\n\")\n",
        "print_policy_grid_arrows(policy_R2_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq2vmy11jjUY",
        "outputId": "00f67dae-e3e1-49c2-b0a4-86082b5406dc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Grid (P2) with obstacle at (3,4):\n",
            "\n",
            " ‚Üò   ‚Üò   ‚Üò   ‚Üì   ‚Üô   ‚Üì   ‚Üô   ‚Üô \n",
            " H   ‚Üí   ‚Üì   ‚Üô   ‚Üì   ‚Üô   ‚Üô   ‚Üô \n",
            " H   ‚Üí   ‚Üò   ‚Üô   ‚Üô   ‚Üô   ‚Üô   ‚Üô \n",
            " ‚Üì   ‚Üò   ‚Üì   ‚Üô   ‚Üê   ‚Üê   ‚Üê   ‚Üê \n",
            " ‚Üí   ‚Üí   G   ‚Üê   ‚Üê   ‚Üê   ‚Üê   ‚Üñ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Whether P2 Reaches the Goal from All S1‚ÄìS7**\n",
        "\n",
        "\n",
        "‚ÄúP2 is compared with P1 and a suitable explanation is given.‚Äù\n",
        "\n",
        "* I will check if adding an obstacle changed reachability.\n",
        "\n"
      ],
      "metadata": {
        "id": "8OSBNOJ0kGZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test P2**"
      ],
      "metadata": {
        "id": "ZS0rzEvBkNfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Testing P2 validity (with obstacle at (3,4)):\\n\")\n",
        "\n",
        "for s in START_STATES:\n",
        "    ok, steps = follow_policy(s, final_policy, obstacles=OBSTACLES_R2)\n",
        "    print(f\"Start {s}: {'Reached goal' if ok else 'FAILED'} in {steps} steps\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7uv27_ikPAs",
        "outputId": "4def1740-2191-42fd-a265-1f978a01a592"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing P2 validity (with obstacle at (3,4)):\n",
            "\n",
            "Start (1, 1): Reached goal in 2 steps\n",
            "Start (2, 1): Reached goal in 3 steps\n",
            "Start (2, 2): Reached goal in 1 steps\n",
            "Start (3, 2): Reached goal in 3 steps\n",
            "Start (4, 2): Reached goal in 4 steps\n",
            "Start (5, 2): Reached goal in 4 steps\n",
            "Start (5, 3): Reached goal in 4 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P2 also successfully reaches the goal from ALL start states, even with the obstacle at (3,4).\n",
        "\n",
        "This means:\n",
        "\n",
        "* P1 works\n",
        "* P2 works\n",
        "* The obstacle changes the shape of the policy, but does NOT break goal reachability"
      ],
      "metadata": {
        "id": "hHWb_0aCkmB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "\n",
        "**R2: Comparison Between P1 and P2**\n",
        "\n",
        "The optimal policy P1 was computed for the original environment without any additional obstacles, using the discovered live-in reward r = ‚Äì20. The resulting policy directs the agent toward the goal at (1,3) while avoiding the hazard states at (3,1) and (4,1). The arrows in P1 show smooth diagonal and downward movements funneling toward the goal.\n",
        "\n",
        "\n",
        "\n",
        "When a new obstacle was introduced at (3,4) to form the modified environment for P2, the value iteration process was recomputed using the same live-in reward r = ‚Äì20. The resulting policy P2 still enables the agent to reach the goal from all seven starting cells (S1‚ÄìS7), but the shape of the policy around column 4 changed noticeably.\n",
        "\n",
        "\n",
        "\n",
        "Specifically, in P1 the preferred movement from states near (3,4) involved direct diagonal moves toward the goal. However, in P2, the obstacle at (3,4) blocks this route, forcing the optimal policy to reroute around the obstacle. This produces changes such as:\n",
        "\n",
        "* More downward (‚Üì) and leftward (‚Üê) arrows around the (3,4) region\n",
        "\n",
        "* A visible ‚Äúdetour‚Äù path compared to the more direct trajectory in P1\n",
        "\n",
        "* Altered diagonal movement patterns in rows 2‚Äì4\n",
        "\n",
        "\n",
        "\n",
        "Despite these changes in local movement, the global behavior of the policy remains unchanged: all paths still converge toward the goal, and all start states successfully reach the goal within a small number of steps. This demonstrates that the obstacle influences local decision-making, but the overall optimal long-term objective remains identical.\n",
        "\n",
        "\n",
        "\n",
        "Thus, P1 and P2 differ in the fine-grained structure of the path but are consistent in achieving the task goal."
      ],
      "metadata": {
        "id": "BBCW-_wLk0m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending the MDP to Prevent Collisions Between Multiple Robots:\n",
        "\n",
        "\n",
        "**Explanation **\n",
        "\n",
        "* In a multi-robot navigation setting, the main challenge is that the movement of each robot is no longer independent. The transition probabilities for one robot now depend on the predicted positions of all other robots. To extend the MDP framework, we incorporate the idea of dynamic occupancy probability: each neighbor cell is assigned a probability of being occupied by another robot at time ùë° + 1. This probability modifies the transition model so that the agent avoids moving into cells with high collision likelihood.\n",
        "\n",
        "* Concretely, instead of using a static transition function\n",
        "ùëá\n",
        "(\n",
        "ùë†\n",
        ",\n",
        "ùëé\n",
        ",\n",
        "ùë†\n",
        "‚Ä≤\n",
        ")\n",
        ", we define a new transition function:\n",
        "\n",
        "`T ‚Ä≤ ( s , a , s ‚Ä≤ ) = T ( s , a , s ‚Ä≤ ) √ó ( 1 ‚àí Poccupied‚Äã ( s ‚Ä≤ ) )`\n",
        "\n",
        "\n",
        "\n",
        "* where\n",
        "ùëÉ\n",
        "occupied\n",
        "(\n",
        "ùë†\n",
        "‚Ä≤\n",
        ")\n",
        " is the predicted probability that another robot will be in state\n",
        "ùë†\n",
        "‚Ä≤\n",
        "s\n",
        "‚Ä≤\n",
        ". If a neighboring cell has a high occupancy probability, its effective transition probability is reduced, and more probability mass shifts to safer cells or the fallback ‚Äústay in place‚Äù transition. This modification integrates collision avoidance directly into the MDP‚Äôs Bellman update process.\n",
        "\n",
        "* Additionally, multi-robot coordination requires modeling the joint state of all robots or using decentralized approximations. A full joint MDP becomes intractable because the state space grows exponentially with the number of robots, so practical systems use factored MDPs, local interaction zones, or priority rules (e.g., robots yield based on ID or direction). These strategies limit computation while still ensuring safety. Under this extended framework, the optimal policy not only minimizes cost and reaches the goal but also avoids collisions by selecting actions with minimal occupancy risk."
      ],
      "metadata": {
        "id": "echbMKDslY6c"
      }
    }
  ]
}